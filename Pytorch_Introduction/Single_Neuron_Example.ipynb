{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single neuron with PyTorch\n",
    "In the following code we will define and train a single neuron model in order to predict the y value of a linear function given an x value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem we want to solve\n",
    "First, we define the linear function that the neuron will try to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network\n",
    "Now we need to define the neuron. We will do this by using the **torch.nn** library. This library contains the **Module** class. Our model (in this case a neuron) will always be a subclass of this class. The **torch.nn** library also contains definitions for different types of layers, such as fully connected linear layers or convolutional layers. It also contains different loss criterions and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Here we define a neural network class called Net. When defining a neural network, it always has to be a subclass of nn.Module\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Here we define the layers of our neural network.\n",
    "        # Since we want only one neuron, we use a linear layer with one input and one output\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "    \n",
    "    # All the neural networks that we define must have a forward function.\n",
    "    # This function propagates the input through the neural network and produce the output\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a class **Net()** with the desired structure, we can define an instance of this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the weights and biases of our neural network by using **state_dict()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc.weight', tensor([[0.0838]])), ('fc.bias', tensor([-0.0069]))])\n"
     ]
    }
   ],
   "source": [
    "print(neuron.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, since the parameters of the neuron are initiated randomly, the neuron will not be able to predict the correct output given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value: 7.56 || Prediction: 0.27\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(5)\n",
    "target = f(x)\n",
    "output = neuron(torch.Tensor([x]))\n",
    "print('Real value: {:.2f} || Prediction: {:.2f}'.format(target, output.detach().numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to produce an (incorrect) output given an input, we need a criterion to tell the neural network how good or bad was the prediction that it made. There are several loss functions available, but we will start by using the simplest one which is the mean squared error loss, given by\n",
    "\\begin{equation}\n",
    "\\text{MSE}(\\text{output}, \\text{target}) = (\\text{output}-\\text{target})^2\n",
    "\\end{equation}\n",
    "**torch.nn** includes the definition of this loss function as **nn.MSELoss()**. We will define criterion as the **nn.MSELoss()** function to make it more explicit that when we are using this function, it is because that is our criterion of how good/bad was the output produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a neural network with PyTorch, the tensors used keep track of all the operations that have been performed on them. Thanks to this information we can use the loss function obtained (which has all the information of what operations have been performed by the neural network to finally obtain the loss) in order to perform all the gradients that will be needed for training of the neural network.\n",
    "\n",
    "At this point we know how to produce an output with our neural network and we have a criterion of how good was the output obtained with a loss function. Now we need to backpropagate the loss through the neural network. For this, we will use the standard gradient descent, which will calculate derivatives in order to change the parameters in the direction that reduces the value of the loss\n",
    "\\begin{equation}\n",
    "\\text{parameter} = \\text{parameter} - \\gamma\\frac{\\partial L}{\\partial (\\text{parameter})}\n",
    "\\end{equation}\n",
    "where $\\gamma$ is the so-called learning rate. Selecting an appropiate value for the learning rate is important since a low value would result in a slow training and a high value would yield parameters corrections that make the loss value jump around instead of getting closer to a minimum.\n",
    "\n",
    "**torch.optim** has different optimizers, but we will use the SGD for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Define the optimizer to be used.\n",
    "#The SGD takes as arguments the parameters of our neural network and a value for the learning rate.\n",
    "optimizer = optim.SGD(neuron.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neuron\n",
    "We can train the neuron with a simple loop over the number of points that we want to sample for the training process. The loop will perform the following steps:\n",
    "1. Since the backward gradient calculation method adds the newly calculated gradient to the previous gradient and we do not want this, we will start each step by setting the gradients to zero.\n",
    "2. We select a random value of $x$ and evaluate the function and the neuron for that input.\n",
    "3. We obtain the loss by comparing the target value and the output of our neuron by using our criterion (in this case MSELoss).\n",
    "4. Once we have the loss, we can call the **backward()** function to calculate all the gradients based on the loss obtained.\n",
    "5. With the gradients calculated, we can use the **step()** function of our optimizer to perform one step of the gradient descent based on the gradients and our learning parameter.\n",
    "6. With the previous steps we have performed a small correction of the parameters of the neuron. Repeating these steps several times should converge to the correct weight and bias that models the target linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0|| weight:tensor([0.2515]), bias:tensor([0.0839])\n",
      "Iteration 200|| weight:tensor([2.0809]), bias:tensor([0.7398])\n",
      "Iteration 400|| weight:tensor([2.0548]), bias:tensor([0.8467])\n",
      "Iteration 600|| weight:tensor([2.0325]), bias:tensor([0.9118])\n",
      "Iteration 800|| weight:tensor([2.0143]), bias:tensor([0.9453])\n",
      "Iteration 1000|| weight:tensor([2.0104]), bias:tensor([0.9674])\n",
      "Iteration 1200|| weight:tensor([2.0074]), bias:tensor([0.9788])\n",
      "Iteration 1400|| weight:tensor([2.0036]), bias:tensor([0.9864])\n",
      "Iteration 1600|| weight:tensor([2.0029]), bias:tensor([0.9912])\n",
      "Iteration 1800|| weight:tensor([2.0016]), bias:tensor([0.9945])\n",
      "Iteration 2000|| weight:tensor([2.0010]), bias:tensor([0.9967])\n",
      "Iteration 2200|| weight:tensor([2.0006]), bias:tensor([0.9980])\n",
      "Iteration 2400|| weight:tensor([2.0004]), bias:tensor([0.9987])\n",
      "Iteration 2600|| weight:tensor([2.0002]), bias:tensor([0.9993])\n",
      "Iteration 2800|| weight:tensor([2.0001]), bias:tensor([0.9996])\n"
     ]
    }
   ],
   "source": [
    "# We sample for 3000 points\n",
    "for i in range(3000):\n",
    "    # The first step is to set to zero all the previously calculated gradients\n",
    "    optimizer.zero_grad()\n",
    "    # We will sample points from the [0,5] interval\n",
    "    input = np.random.uniform(5)\n",
    "    # Evaluate f(x) for our current point\n",
    "    target = f(input)\n",
    "    # Pass our current point through the neuron to obtain the output\n",
    "    output = neuron(torch.Tensor([input]))\n",
    "    # Compare the output with the target to obtain the loss\n",
    "    loss = criterion(output, torch.Tensor([target]))\n",
    "    # Calculate the gradients based on the obtained loss\n",
    "    loss.backward()\n",
    "    # Update the parameters by SGD based on the gradients obtained\n",
    "    optimizer.step()\n",
    "    # Print the parameters every 200 samples\n",
    "    if i%200 == 0:\n",
    "        print('Iteration {}|| weight:{}, bias:{}'.format(i, neuron.state_dict()['fc.weight'][0], neuron.state_dict()['fc.bias']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
