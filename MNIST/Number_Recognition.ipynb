{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST database image recognition\n",
    "In this example we will define and train a neural network to recognise handwritten numbers. First, we import the libraries that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the .csv files containing the MNIST labeled handwritten numbers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training data...\n",
      "Loaded training array of dimensions 60000x785\n",
      "Loading the test data...\n",
      "Loaded test array of dimensions 10000x785\n"
     ]
    }
   ],
   "source": [
    "image_size = 28 # width and length of the images in pixels\n",
    "data_path = \"numbers/\" # path of the .csv files with the numbers and labels\n",
    "\n",
    "# We load the training data as a numpy array. This array includes the label in the first position.\n",
    "print('Loading the training data...')\n",
    "train_data = np.loadtxt(data_path + \"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "print('Loaded training array of dimensions {}x{}'.format(np.shape(train_data)[0],np.shape(train_data)[1]))\n",
    "\n",
    "# We load the test data as a numpy array. This array includes the label in the first position.\n",
    "print('Loading the test data...')\n",
    "test_data = np.loadtxt(data_path + \"mnist_test.csv\", \n",
    "                       delimiter=\",\")\n",
    "print('Loaded test array of dimensions {}x{}'.format(np.shape(test_data)[0],np.shape(test_data)[1]))\n",
    "\n",
    "\n",
    "\n",
    "# Translate the labels of each image in the training dataset (number from 0 to 9)\n",
    "# to an array filled with zeros and a 1 in the position of the label\n",
    "auxiliar_train = np.zeros((len(train_data), 10))\n",
    "for i in range(len(train_data)):\n",
    "    value = int(train_data[i,0])\n",
    "    auxiliar_train[i,value] = 1\n",
    "\n",
    "# Translate the labels of each image in the test dataset (number from 0 to 9)\n",
    "# to an array filled with zeros and a 1 in the position of the label\n",
    "auxiliar_test = np.zeros((len(test_data), 10))\n",
    "for i in range(len(test_data)):\n",
    "    value = int(test_data[i,0])\n",
    "    auxiliar_test[i,value] = 1\n",
    "\n",
    "    \n",
    "    \n",
    "# Save the images (normalised) and labels as PyTorch tensors\n",
    "train_labels = torch.Tensor(auxiliar_train)\n",
    "train_images = torch.Tensor(train_data[:,1:])/255.\n",
    "test_labels = torch.Tensor(auxiliar_test)\n",
    "test_images = torch.Tensor(test_data[:,1:])/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dimensions of our train_images array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to show the image of one element of the dataset. We just have to reshape one of the elements of train_images/test_images to dimension 28x28, since in the database they are flattened to be of dimension 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a random element of the database\n",
    "index = np.random.randint(0,60000)\n",
    "plt.gray()\n",
    "# Show the reshaped tensor as a 28x28 image\n",
    "plt.imshow(train_images[index].view(image_size, image_size))\n",
    "print('Label: {}'.format(train_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define our neural network. The process is similar to the previous single neuron case, but now we will have an input of dimension image_size*image_size for the first layer and we will have four hidden layers. The number in the output of one of the hidden layers must be equal to the number in the input of the next hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Hidden layer of 256 neurons\n",
    "            nn.Linear(image_size*image_size, 256),\n",
    "            nn.ReLU(),\n",
    "            # Hidden layer of 128 neurons\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            # Hidden layer of 64 neurons\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            # Output\n",
    "            nn.Linear(64, 10),\n",
    "            )\n",
    "                \n",
    "    def forward(self, input):\n",
    "            output = self.main(input)\n",
    "            output = torch.tanh(output)\n",
    "            return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network is initialised randomly. Thus, if we pass one of the numbers from the dataset through it, the output will be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0174,  0.1559, -0.0175,  0.0738,  0.0313,  0.0226, -0.0994,  0.1603,\n",
      "        -0.0942,  0.0058], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net(train_images[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to define the optimizer (we will use the \"Adam\" optimizer) and the criterion (MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the neural network image by image, we will use batches of several images. We will pass a batch to the neural network, accumulate the gradients, and once the batch is completed we will perform the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example of a batch of 10 images\n",
    "batch = train_images[0:20]\n",
    "# Print the output of the batch through the network\n",
    "print(net(batch).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tensor of twenty results (the batch size that we used) of ten components each (the possible number outputs). Now we have everything ready to perform the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 // Step 599/600\r"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0, len(train_labels), BATCH_SIZE):\n",
    "        batch_images = train_images[i:i+BATCH_SIZE]\n",
    "        batch_labels = train_labels[i:i+BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        \n",
    "        outputs = net(batch_images)\n",
    "        \n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        print('Epoch {} // Step {}/{}'.format(epoch, int(i/BATCH_SIZE), int(len(train_labels)/BATCH_SIZE)), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training we can check the performance of the neural network by using the test dataset that we created. We are going to count the total of correct answers given by the neural network and save the index of the incorrect guesses for posterior visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9771\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_images)):\n",
    "        real_class = torch.argmax(test_labels[i])\n",
    "        out = net(test_images[i])\n",
    "        predicted = torch.argmax(out)\n",
    "        if predicted == real_class:\n",
    "            correct +=1\n",
    "        else:\n",
    "            error.append(i)\n",
    "        total +=1\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value: 9\n",
      "Neural network outcome 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_number = 5\n",
    "plt.imshow(test_images[error[error_number]].view(28,28))\n",
    "print(\"True value: {}\".format(torch.argmax(test_labels[error[error_number]])))\n",
    "print(\"Neural network outcome {}\".format(torch.argmax(net(test_images[error[error_number]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
